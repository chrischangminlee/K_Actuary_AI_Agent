import streamlit as st
import os
from openai import OpenAI
from dotenv import load_dotenv
from pinecone import Pinecone

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(
    api_key=st.secrets["OPENAI_API_KEY"],
    timeout=60.0  # íƒ€ì„ì•„ì›ƒ ì„¤ì • ì¶”ê°€
)

# Pinecone ì´ˆê¸°í™”
pc = Pinecone(api_key=st.secrets["PINECONE_API_KEY"])
index = pc.Index("actuary-docs")

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
SYSTEM_PROMPT = """ë‹¹ì‹ ì€ í•œêµ­ì˜ ê³„ë¦¬ì‚¬ë“¤ì„ ë•ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì „ë¬¸ì„±ê³¼ ì •í™•ì„±ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì›ì¹™ì„ ë”°ë¼ ì‘ë‹µí•´ì£¼ì„¸ìš”:

ì˜¤ì§ ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ê³ , ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì— ëŒ€í•´ì„œëŠ” ë‹µë³€í•˜ì§€ë§ˆì„¸ìš”.
ë‹µë³€ ì‹œ ì°¸ê³ í•œ ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°˜ë“œì‹œ í•´ë‹¹ íŒŒì¼ëª…ê³¼ í˜ì´ì§€ë¥¼ ì¸ìš©í•˜ì—¬ ì„¤ëª…í•´ì£¼ì„¸ìš”. 

ë³´í—˜ë£Œ ì‚°ì¶œ, ì¤€ë¹„ê¸ˆ í‰ê°€, ì†í•´ìœ¨ ê°€ì • ë“± ê³„ë¦¬ì  ê°€ì •ê³¼ ëª¨ë¸ì™€ ê°™ì€ ì‹¤ë¬´ì— í•„ìš”í•œ ì„¤ëª… ì œê³µ

1. ê´€ë ¨ ë²•ê·œì™€ ê·œì •ì„ ê³ ë ¤í•˜ì—¬ ì¡°ì–¸
   - ë³´í—˜ì—…ë²•, ê°ë…ê·œì •, IFRS17 ë“± ê´€ë ¨ ê·œì • ì°¸ì¡°
   - ë²•ê·œ ì¤€ìˆ˜ ì‚¬í•­ ê°•ì¡°

2. ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì— ëŒ€í•´ì„œëŠ” ëª…í™•íˆ í•œê³„ì  ì–¸ê¸‰
   - ì¶”ê°€ ê²€í† ë‚˜ ì „ë¬¸ê°€ í™•ì¸ì´ í•„ìš”í•œ ì‚¬í•­ ëª…ì‹œ
   - ê°€ì •ì´ë‚˜ ì œí•œì‚¬í•­ ëª…í™•íˆ ì„¤ëª…

ì•„ë˜ëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œ ë‚´ìš©ì…ë‹ˆë‹¤:
{context}

ìœ„ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."""

def get_relevant_context(query, top_k=5):
    """ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œ ê²€ìƒ‰"""
    # ì„ë² ë”© ìƒì„±
    response = client.embeddings.create(
        model="text-embedding-ada-002",
        input=query
    )
    query_embedding = response.data[0].embedding
    
    # KIC-S ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸
    kics_keywords = ["KIC-S", "K-ICS", "KICS", "ì§€ê¸‰ì—¬ë ¥"]
    is_kics_related = any(keyword.lower() in query.lower() for keyword in kics_keywords)
    
    # Pinecone ê²€ìƒ‰ - ë” ë§ì€ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜´
    results = index.query(
        vector=query_embedding,
        top_k=top_k * 4,  # ë” ë§ì€ ê²°ê³¼ë¥¼ ê°€ì ¸ì™€ì„œ í•„í„°ë§
        include_metadata=True
    )
    
    # ë””ë²„ê¹…ì„ ìœ„í•´ ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥
    print("\n=== ê²€ìƒ‰ ê²°ê³¼ ===")
    print(f"ì´ {len(results.matches)}ê°œ ê²°ê³¼ ë°œê²¬")
    print(f"KIC-S ê´€ë ¨ ì¿¼ë¦¬: {is_kics_related}")
    
    # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¬¸ì„œë³„ë¡œ ê·¸ë£¹í™”
    doc_groups = {
        "IFRS17ë³´í—˜íšŒê³„í•´ì„¤ì„œ_2022.pdf": [],
        "KICS í•´ì„¤ì„œ.pdf": []
    }
    
    for match in results.matches:
        metadata = match.metadata
        doc_key = metadata['file_name']
        if doc_key in doc_groups:
            doc_groups[doc_key].append((match.score, metadata))
    
    # ê° ë¬¸ì„œì—ì„œ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ê²°ê³¼ ì„ íƒ
    contexts = []
    
    # KIC-S ê´€ë ¨ ì¿¼ë¦¬ì¸ ê²½ìš° KIC-S ë¬¸ì„œì—ì„œ ë” ë§ì€ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜´
    if is_kics_related:
        kics_count = 3
        ifrs_count = 2
    else:
        kics_count = 2
        ifrs_count = 3
    
    # KIC-S í•´ì„¤ì„œì—ì„œ ê²°ê³¼ ì„ íƒ
    kics_matches = sorted(doc_groups["KICS í•´ì„¤ì„œ.pdf"], key=lambda x: x[0], reverse=True)[:kics_count]
    for score, metadata in kics_matches:
        if score > 0.5:  # ìœ ì‚¬ë„ê°€ ì¼ì • ìˆ˜ì¤€ ì´ìƒì¸ ê²½ìš°ë§Œ í¬í•¨
            print(f"\n[ê²°ê³¼] íŒŒì¼: {metadata['file_name']}, í˜ì´ì§€: {metadata['page']}, ìœ ì‚¬ë„: {score}")
            contexts.append(
                f"[{metadata['file_name']} - {metadata['page']}í˜ì´ì§€]\n{metadata['text']}\n"
            )
    
    # IFRS17 í•´ì„¤ì„œì—ì„œ ê²°ê³¼ ì„ íƒ
    ifrs_matches = sorted(doc_groups["IFRS17ë³´í—˜íšŒê³„í•´ì„¤ì„œ_2022.pdf"], key=lambda x: x[0], reverse=True)[:ifrs_count]
    for score, metadata in ifrs_matches:
        if score > 0.5:  # ìœ ì‚¬ë„ê°€ ì¼ì • ìˆ˜ì¤€ ì´ìƒì¸ ê²½ìš°ë§Œ í¬í•¨
            print(f"\n[ê²°ê³¼] íŒŒì¼: {metadata['file_name']}, í˜ì´ì§€: {metadata['page']}, ìœ ì‚¬ë„: {score}")
            contexts.append(
                f"[{metadata['file_name']} - {metadata['page']}í˜ì´ì§€]\n{metadata['text']}\n"
            )
    
    if not contexts:  # ìœ ì‚¬ë„ê°€ ë„ˆë¬´ ë‚®ì•„ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°
        # ìœ ì‚¬ë„ ê¸°ì¤€ì„ ë‚®ì¶°ì„œ ë‹¤ì‹œ ì‹œë„
        for matches in [kics_matches, ifrs_matches]:
            for score, metadata in matches:
                print(f"\n[ê²°ê³¼] íŒŒì¼: {metadata['file_name']}, í˜ì´ì§€: {metadata['page']}, ìœ ì‚¬ë„: {score}")
                contexts.append(
                    f"[{metadata['file_name']} - {metadata['page']}í˜ì´ì§€]\n{metadata['text']}\n"
                )
                if len(contexts) >= 3:
                    break
    
    return "\n\n".join(contexts[:5])

def get_ai_response(query, temperature=0.7):
    """OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±"""
    # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    context = get_relevant_context(query)
    
    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ì²« ë²ˆì§¸ë¡œ ì¶”ê°€
    messages = [{"role": "system", "content": SYSTEM_PROMPT.format(context=context)}]
    
    # ìµœê·¼ 10ê°œì˜ ëŒ€í™” ê¸°ë¡ì„ ì¶”ê°€
    recent_messages = st.session_state.messages[-10:] if len(st.session_state.messages) > 10 else st.session_state.messages
    messages.extend(recent_messages)
    
    # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
    messages.append({"role": "user", "content": query})
    
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=messages,
        temperature=temperature,
        max_tokens=2000
    )
    
    return response.choices[0].message.content

def initialize_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    if 'messages' not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”, K-Actuary AI Assistantì…ë‹ˆë‹¤. ë³´í—˜ê³„ë¦¬ ê´€ë ¨ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë‹¤ë©´ ë§ì”€í•´ì£¼ì„¸ìš”."}
        ]
    if 'temperature' not in st.session_state:
        st.session_state.temperature = 0.7

def set_custom_theme():
    """ì»¤ìŠ¤í…€ í…Œë§ˆ ì„¤ì •"""
    st.markdown("""
        <style>
        .stApp {
            background-color: #f5f5f5;
        }
        .css-1d391kg {
            padding: 1rem 1rem;
        }
        .stChatMessage {
            background-color: white;
            border-radius: 15px;
            padding: 1rem;
            margin: 0.5rem 0;
        }
        </style>
    """, unsafe_allow_html=True)

def main():
    st.set_page_config(
        page_title="K Actuary AI Agent",
        page_icon="ğŸ¤–",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # ì»¤ìŠ¤í…€ í…Œë§ˆ ì ìš©
    set_custom_theme()
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    initialize_session_state()
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.markdown("### ì†Œê°œ")
        st.markdown("""
        - ë³¸ AI ì±— ì„œë¹„ìŠ¤ëŠ” í•œêµ­ ê³„ë¦¬ì—…ë¬´ë¥¼ ìˆ˜í–‰í•˜ëŠ” ê³„ë¦¬ì‚¬ë¥¼ ìœ„í•´ ê°œë°œëœ ê°œì¸ í”„ë¡œì íŠ¸ ê¸°ë°˜ AI Chat / Agentì…ë‹ˆë‹¤.
        - í˜„ì¬ ë‹¤ì–‘í•œ ìœ ìš©í•œ ê¸°ëŠ¥ë“¤ì´ ì§€ì†ì ìœ¼ë¡œ ê°œë°œ ì¤‘ì´ë©°, ë³´ë‹¤ í–¥ìƒëœ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ê¸° ìœ„í•´ ê°œì„ ì„ ì´ì–´ê°€ê³  ìˆìŠµë‹ˆë‹¤.
        """)
        st.markdown('<p style="color: red; font-size: 12px;">- (ì£¼ì˜) ë³¸ AIê°€ ì œê³µí•˜ëŠ” ë‹µë³€ì€ ì°¸ê³ ìš©ì´ë©°, ì •í™•ì„±ì„ ë³´ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë³´ì•ˆì„ ìœ„í•´ íšŒì‚¬ ê¸°ë°€, ê°œì¸ì •ë³´ë“±ì€ ì œê³µí•˜ì§€ ì•Šê¸°ë¥¼ ê¶Œì¥ë“œë¦¬ë©°, ë°˜ë“œì‹œ ì‹¤ì œ ì—…ë¬´ì— ì ìš©í•˜ê¸° ì „ì— ê²€í† í•˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤.</p>', unsafe_allow_html=True)
        st.markdown("---")
        st.markdown("### ì°¸ê³ ëœ pdf")
        st.markdown("""
        - IFRS17ë³´í—˜íšŒê³„í•´ì„¤ì„œ_2022.pdf
        - KIC-S í•´ì„¤ì„œ.pdf
        """)
        st.markdown("---")
        st.markdown('[ê°œë°œì Linkedin](https://www.linkedin.com/in/chrislee9407/)')
        st.markdown('[K-ê³„ë¦¬ AI í”Œë«í¼](https://chrischangminlee.github.io/K_Actuary_AI_Agent_Platform/)')
    
    # ë©”ì¸ ì˜ì—­
    st.title("K Actuary AI Agent")
    st.markdown("---")
    
    # ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.write(message["content"])
    
    # ì‚¬ìš©ì ì…ë ¥
    if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.write(prompt)
        
        # AI ì‘ë‹µ ìƒì„±
        with st.chat_message("assistant"):
            with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                response = get_ai_response(prompt, st.session_state.temperature)
                st.write(response)
                st.session_state.messages.append({"role": "assistant", "content": response})

if __name__ == "__main__":
    main() 