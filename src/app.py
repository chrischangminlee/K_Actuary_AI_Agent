import streamlit as st
import os
from openai import OpenAI
from dotenv import load_dotenv
from pinecone import Pinecone

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(
    api_key=st.secrets["OPENAI_API_KEY"],
    timeout=60.0  # íƒ€ì„ì•„ì›ƒ ì„¤ì • ì¶”ê°€
)

# Pinecone ì´ˆê¸°í™”
pc = Pinecone(api_key=st.secrets["PINECONE_API_KEY"])
index = pc.Index("actuary-docs")

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
SYSTEM_PROMPT = """ë‹¹ì‹ ì€ í•œêµ­ì˜ ê³„ë¦¬ì‚¬ë“¤ì„ ì§€ì›í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.  
ì•„ë˜ ê·œì¹™ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•´ì£¼ì„¸ìš”:

1. **ì¶œì²˜ í‘œê¸° í•„ìˆ˜:**  
   - ë‹µë³€ì˜ ì²« ë¶€ë¶„ì— ë°˜ë“œì‹œ "íŒŒì¼ëª… - í˜ì´ì§€" í˜•ì‹ìœ¼ë¡œ ì¶œì²˜ë¥¼ ê¸°ì¬í•  ê²ƒ.
2. **ë¬¸ì„œ ê¸°ë°˜ ì‘ë‹µ:**  
   - ì œê³µëœ ë¬¸ì„œ({context})ì— í¬í•¨ëœ ë‚´ìš©ë§Œì„ í™œìš©í•˜ì—¬ ë‹µë³€í•  ê²ƒ.
   - ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ê°€í•˜ì§€ ì•Šì„ ê²ƒ.
3. **ì‹¤ë¬´ ê´€ë ¨ ì„¤ëª…:**  
   - ë³´í—˜ë£Œ ì‚°ì¶œ, ì¤€ë¹„ê¸ˆ í‰ê°€, ì†í•´ìœ¨ ê°€ì • ë“± ê³„ë¦¬ ì‹¤ë¬´ ê´€ë ¨ ë‚´ìš© í¬í•¨.
4. **ë²•ê·œ ë° ê·œì • ì¤€ìˆ˜:**  
   - ë³´í—˜ì—…ë²•, ê°ë…ê·œì •, IFRS17 ë“± ê´€ë ¨ ê·œì •ì„ ì°¸ê³ í•˜ê³ , ë²•ê·œ ì¤€ìˆ˜ ì‚¬í•­ ê°•ì¡°.
5. **ë¶ˆí™•ì‹¤í•œ ì‚¬í•­ ëª…ì‹œ:**  
   - ë¶ˆí™•ì‹¤í•˜ê±°ë‚˜ ì¶”ê°€ ê²€í† ê°€ í•„ìš”í•œ ê²½ìš°, í•œê³„ì  ë° ì£¼ì˜ì‚¬í•­ì„ ëª…í™•íˆ ì–¸ê¸‰.

ì•„ë˜ëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œ ë‚´ìš©ì…ë‹ˆë‹¤:
{context}

ìœ„ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."""

def get_relevant_context(query, top_k=5):
    """ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œ ê²€ìƒ‰"""
    # ì„ë² ë”© ìƒì„±
    response = client.embeddings.create(
        model="text-embedding-ada-002",
        input=query
    )
    query_embedding = response.data[0].embedding
    
    # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¬¸ì„œë³„ë¡œ ê·¸ë£¹í™”
    doc_groups = {
        "IFRS17ë³´í—˜íšŒê³„í•´ì„¤ì„œ_2022.pdf": [],
        "KICS í•´ì„¤ì„œ.pdf": [],
        "ë³´í—˜ê°œë°œì›_20200220_ì¼ë°˜ì†ë³´ ìœ„í—˜ì¡°ì • ì ìš©ê¸°ë²• ê³ ë„í™”.pdf": [],
        "ë³´í—˜ê°œë°œì›_202203_IFRS17 ê²½ì œì  ê°€ì • ì‹¤ë¬´ì ìš©ë°©ì•ˆ.pdf": [],
        "ê¸ˆê°ì›_230302ê³µë™ì¬ë³´í—˜ ë° ì¬ë³´í—˜ ë°ì´í„° ì œê³µ ê´€ë ¨ ì—…ë¬´ì²˜ë¦¬ ê°€ì´ë“œë¼ì¸.pdf": [],
        "ê¸ˆìœµìœ„_241106_IFRS17 ì£¼ìš” ê³„ë¦¬ê°€ì • ê°€ì´ë“œë¼ì¸.pdf": []
    }
    
    # KIC-S ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸
    kics_keywords = ["KIC-S", "K-ICS", "KICS", "ì§€ê¸‰ì—¬ë ¥"]
    ifrs_keywords = ["IFRS", "IFRS17", "ê²½ì œì  ê°€ì •", "ê³„ë¦¬ê°€ì •"]
    reinsurance_keywords = ["ì¬ë³´í—˜", "ê³µë™ì¬ë³´í—˜"]
    risk_adjustment_keywords = ["ìœ„í—˜ì¡°ì •", "ë¦¬ìŠ¤í¬ë§ˆì§„"]
    
    # í‚¤ì›Œë“œ ë§¤ì¹­ì„ í†µí•œ ë¬¸ì„œ ìš°ì„ ìˆœìœ„ ê²°ì •
    is_kics_related = any(keyword.lower() in query.lower() for keyword in kics_keywords)
    is_ifrs_related = any(keyword.lower() in query.lower() for keyword in ifrs_keywords)
    is_reinsurance_related = any(keyword.lower() in query.lower() for keyword in reinsurance_keywords)
    is_risk_adjustment_related = any(keyword.lower() in query.lower() for keyword in risk_adjustment_keywords)
    
    # Pinecone ê²€ìƒ‰ - ë” ë§ì€ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜´
    results = index.query(
        vector=query_embedding,
        top_k=top_k * 4,  # ë” ë§ì€ ê²°ê³¼ë¥¼ ê°€ì ¸ì™€ì„œ í•„í„°ë§
        include_metadata=True
    )
    
    # ë””ë²„ê¹…ì„ ìœ„í•´ ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥
    print("\n=== ê²€ìƒ‰ ê²°ê³¼ ===")
    print(f"ì´ {len(results.matches)}ê°œ ê²°ê³¼ ë°œê²¬")
    print(f"KIC-S ê´€ë ¨ ì¿¼ë¦¬: {is_kics_related}")
    
    for match in results.matches:
        metadata = match.metadata
        doc_key = metadata['file_name']
        if doc_key in doc_groups:
            doc_groups[doc_key].append((match.score, metadata))
    
    # ê° ë¬¸ì„œì—ì„œ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ê²°ê³¼ ì„ íƒ
    contexts = []
    target_counts = {
        "KICS í•´ì„¤ì„œ.pdf": 3 if is_kics_related else 1,
        "IFRS17ë³´í—˜íšŒê³„í•´ì„¤ì„œ_2022.pdf": 3 if is_ifrs_related else 1,
        "ë³´í—˜ê°œë°œì›_20200220_ì¼ë°˜ì†ë³´ ìœ„í—˜ì¡°ì • ì ìš©ê¸°ë²• ê³ ë„í™”.pdf": 3 if is_risk_adjustment_related else 1,
        "ë³´í—˜ê°œë°œì›_202203_IFRS17 ê²½ì œì  ê°€ì • ì‹¤ë¬´ì ìš©ë°©ì•ˆ.pdf": 3 if is_ifrs_related else 1,
        "ê¸ˆê°ì›_230302ê³µë™ì¬ë³´í—˜ ë° ì¬ë³´í—˜ ë°ì´í„° ì œê³µ ê´€ë ¨ ì—…ë¬´ì²˜ë¦¬ ê°€ì´ë“œë¼ì¸.pdf": 3 if is_reinsurance_related else 1,
        "ê¸ˆìœµìœ„_241106_IFRS17 ì£¼ìš” ê³„ë¦¬ê°€ì • ê°€ì´ë“œë¼ì¸.pdf": 3 if is_ifrs_related else 1
    }
    
    # ê° ë¬¸ì„œì—ì„œ ê²°ê³¼ ì„ íƒ
    for doc_name, count in target_counts.items():
        matches = sorted(doc_groups[doc_name], key=lambda x: x[0], reverse=True)[:count]
        for score, metadata in matches:
            if score > 0.5:  # ìœ ì‚¬ë„ê°€ ì¼ì • ìˆ˜ì¤€ ì´ìƒì¸ ê²½ìš°ë§Œ í¬í•¨
                print(f"\n[ê²°ê³¼] íŒŒì¼: {metadata['file_name']}, í˜ì´ì§€: {metadata['page']}, ìœ ì‚¬ë„: {score}")
                contexts.append(
                    f"[{metadata['file_name']} - {metadata['page']}í˜ì´ì§€]\n{metadata['text']}\n"
                )
    
    if not contexts:  # ìœ ì‚¬ë„ê°€ ë„ˆë¬´ ë‚®ì•„ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°
        # ìœ ì‚¬ë„ ê¸°ì¤€ì„ ë‚®ì¶°ì„œ ë‹¤ì‹œ ì‹œë„
        for doc_name, matches in doc_groups.items():
            sorted_matches = sorted(matches, key=lambda x: x[0], reverse=True)[:1]
            for score, metadata in sorted_matches:
                print(f"\n[ê²°ê³¼] íŒŒì¼: {metadata['file_name']}, í˜ì´ì§€: {metadata['page']}, ìœ ì‚¬ë„: {score}")
                contexts.append(
                    f"[{metadata['file_name']} - {metadata['page']}í˜ì´ì§€]\n{metadata['text']}\n"
                )
                if len(contexts) >= 3:
                    break
    
    return "\n\n".join(contexts[:5])

def get_ai_response(query, temperature=0.7):
    """OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±"""
    # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    context = get_relevant_context(query)
    
    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ì²« ë²ˆì§¸ë¡œ ì¶”ê°€
    messages = [{"role": "system", "content": SYSTEM_PROMPT.format(context=context)}]
    
    # ìµœê·¼ 10ê°œì˜ ëŒ€í™” ê¸°ë¡ì„ ì¶”ê°€
    recent_messages = st.session_state.messages[-10:] if len(st.session_state.messages) > 10 else st.session_state.messages
    messages.extend(recent_messages)
    
    # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
    messages.append({"role": "user", "content": query})
    
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=messages,
        temperature=temperature,
        max_tokens=2000
    )
    
    return response.choices[0].message.content

def initialize_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    if 'messages' not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”, K-Actuary AI Assistantì…ë‹ˆë‹¤. ë³´í—˜ê³„ë¦¬ ê´€ë ¨ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë‹¤ë©´ ë§ì”€í•´ì£¼ì„¸ìš”. ì¢Œì¸¡ ì°¸ê³ ëœ pdfì˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€ì„ ë“œë¦½ë‹ˆë‹¤."}
        ]
    if 'temperature' not in st.session_state:
        st.session_state.temperature = 0.7

def set_custom_theme():
    """ì»¤ìŠ¤í…€ í…Œë§ˆ ì„¤ì •"""
    st.markdown("""
        <style>
        .stApp {
            background-color: #f5f5f5;
        }
        .css-1d391kg {
            padding: 1rem 1rem;
        }
        .stChatMessage {
            background-color: white;
            border-radius: 15px;
            padding: 1rem;
            margin: 0.5rem 0;
        }
        </style>
    """, unsafe_allow_html=True)

def main():
    st.set_page_config(
        page_title="K Actuary AI Agent",
        page_icon="ğŸ¤–",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # ì»¤ìŠ¤í…€ í…Œë§ˆ ì ìš©
    set_custom_theme()
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    initialize_session_state()
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.markdown("### ì†Œê°œ")
        st.markdown("""
        - ë³¸ AI ì±— ì„œë¹„ìŠ¤ëŠ” í•œêµ­ ê³„ë¦¬ì—…ë¬´ë¥¼ ìˆ˜í–‰í•˜ëŠ” ê³„ë¦¬ì‚¬ë¥¼ ìœ„í•´ ê°œë°œëœ ê°œì¸ í”„ë¡œì íŠ¸ë¡œ ë§Œë“¤ì–´ì§„ AI Chatbot / Agentì…ë‹ˆë‹¤.
        - RAG(ê²€ìƒ‰ì¦ê°•ìƒì„±)ì— ë”ë¶ˆì–´ í˜„ì¬ ë‹¤ì–‘í•œ ìœ ìš©í•œ ê¸°ëŠ¥ë“¤ì´ ì§€ì†ì ìœ¼ë¡œ ê°œë°œ ì¤‘ì´ë©°, ë³´ë‹¤ í–¥ìƒëœ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ê¸° ìœ„í•´ ê°œì„ ì„ ì´ì–´ê°€ê³  ìˆìŠµë‹ˆë‹¤.
        """)
        st.markdown('<p style="color: black; font-size: 12px;">- í˜„ì¬ API ë¹„ìš©ì´ ê°€ì¥ ì €ë ´í•œ LLMì¸ gpt3.5-turboë¥¼ ì‚¬ìš©í•˜ê³  ìˆì–´ ê¸°ëŒ€ë³´ë‹¤ ì„±ëŠ¥ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>', unsafe_allow_html=True)
        st.markdown('<p style="color: red; font-size: 12px;">* (ì£¼ì˜) ë³¸ AIê°€ ì œê³µí•˜ëŠ” ë‹µë³€ì€ ì°¸ê³ ìš©ì´ë©°, ì •í™•ì„±ì„ ë³´ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë³´ì•ˆì„ ìœ„í•´ íšŒì‚¬ ê¸°ë°€, ê°œì¸ì •ë³´ë“±ì€ ì œê³µí•˜ì§€ ì•Šê¸°ë¥¼ ê¶Œì¥ë“œë¦¬ë©°, ë°˜ë“œì‹œ ì‹¤ì œ ì—…ë¬´ì— ì ìš©í•˜ê¸° ì „ì— ê²€í† í•˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤.</p>', unsafe_allow_html=True)
        st.markdown("---")
        st.markdown('[ê°œë°œì Linkedin](https://www.linkedin.com/in/chrislee9407/)')
        st.markdown('[K-ê³„ë¦¬ AI í”Œë«í¼](https://chrischangminlee.github.io/K_Actuary_AI_Agent_Platform/)')
        
        # ì´ë¯¸ì§€ í‘œì‹œ
        try:
            st.markdown("### ì±—ë´‡ ê°œë°œ êµ¬ì¡°ë„")
            st.image("KActuaryAgentStructure_021425.png", use_column_width=True, caption="ìš°ì¸¡ í™•ëŒ€ ë²„íŠ¼ í´ë¦­í•˜ì—¬ í¬ê²Œ ë³´ê¸°")
        except Exception as e:
            st.warning("êµ¬ì¡°ë„ ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        st.markdown("---")
        st.markdown("### ì°¸ê³ ëœ pdf")
        st.markdown("""
        - IFRS17ë³´í—˜íšŒê³„í•´ì„¤ì„œ_2022.pdf
        - KICS í•´ì„¤ì„œ.pdf
        - ë³´í—˜ê°œë°œì›_20200220_ì¼ë°˜ì†ë³´ ìœ„í—˜ì¡°ì • ì ìš©ê¸°ë²• ê³ ë„í™”.pdf
        - ë³´í—˜ê°œë°œì›_202203_IFRS17 ê²½ì œì  ê°€ì • ì‹¤ë¬´ì ìš©ë°©ì•ˆ.pdf
        - ê¸ˆê°ì›_230302ê³µë™ì¬ë³´í—˜ ë° ì¬ë³´í—˜ ë°ì´í„° ì œê³µ ê´€ë ¨ ì—…ë¬´ì²˜ë¦¬ ê°€ì´ë“œë¼ì¸.pdf
        - ê¸ˆìœµìœ„_241106_IFRS17 ì£¼ìš” ê³„ë¦¬ê°€ì • ê°€ì´ë“œë¼ì¸.pdf
        """)
        
    
    # ë©”ì¸ ì˜ì—­
    st.title("K Actuary AI Agent")
    st.markdown("---")
    
    # ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.write(message["content"])
    
    # ì‚¬ìš©ì ì…ë ¥
    if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.write(prompt)
        
        # AI ì‘ë‹µ ìƒì„±
        with st.chat_message("assistant"):
            with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                response = get_ai_response(prompt, st.session_state.temperature)
                st.write(response)
                st.session_state.messages.append({"role": "assistant", "content": response})

if __name__ == "__main__":
    main() 