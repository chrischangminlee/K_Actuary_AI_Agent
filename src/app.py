import streamlit as st
import os
from openai import OpenAI
from pinecone import Pinecone
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

# Pinecone ì´ˆê¸°í™”
pinecone = Pinecone(api_key=os.getenv('PINECONE_API_KEY'))
index = pinecone.Index('actuary-docs')

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
SYSTEM_PROMPT = """ë‹¹ì‹ ì€ í•œêµ­ì˜ ê³„ë¦¬ì‚¬ë“¤ì„ ë•ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì „ë¬¸ì„±ê³¼ ì •í™•ì„±ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì›ì¹™ì„ ë”°ë¼ ì‘ë‹µí•´ì£¼ì„¸ìš”:

ì˜¤ì§ ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ê³ , ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì— ëŒ€í•´ì„œëŠ” ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.
ë‹µë³€ ì‹œ ì°¸ê³ í•œ ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°˜ë“œì‹œ í•´ë‹¹ íŒŒì¼ëª…ê³¼ í˜ì´ì§€ë¥¼ ì¸ìš©í•˜ì—¬ ì„¤ëª…í•´ì£¼ì„¸ìš”.

ë³´í—˜ë£Œ ì‚°ì¶œ, ì¤€ë¹„ê¸ˆ í‰ê°€, ì†í•´ìœ¨ ê°€ì • ë“± ê³„ë¦¬ì  ê°€ì •ê³¼ ëª¨ë¸ê³¼ ê°™ì€ ì‹¤ë¬´ì— í•„ìš”í•œ ì„¤ëª…ì„ ì œê³µí•˜ì„¸ìš”.

1. ê´€ë ¨ ë²•ê·œì™€ ê·œì •ì„ ê³ ë ¤í•˜ì—¬ ì¡°ì–¸
   - ë³´í—˜ì—…ë²•, ê°ë…ê·œì •, IFRS17 ë“± ê´€ë ¨ ê·œì • ì°¸ì¡°
   - ë²•ê·œ ì¤€ìˆ˜ ì‚¬í•­ ê°•ì¡°

2. ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì— ëŒ€í•´ì„œëŠ” ëª…í™•íˆ í•œê³„ì  ì–¸ê¸‰
   - ì¶”ê°€ ê²€í† ë‚˜ ì „ë¬¸ê°€ í™•ì¸ì´ í•„ìš”í•œ ì‚¬í•­ ëª…ì‹œ
   - ê°€ì •ì´ë‚˜ ì œí•œì‚¬í•­ ëª…í™•íˆ ì„¤ëª…"""

def query_pinecone(query):
    """Pineconeì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰"""
    # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    response = client.embeddings.create(
        model="text-embedding-ada-002",
        input=query
    )
    query_embedding = response.data[0].embedding
    
    # Pinecone ê²€ìƒ‰
    results = index.query(
        vector=query_embedding,
        top_k=3,
        include_metadata=True
    )
    
    # ê²€ìƒ‰ ê²°ê³¼ í…ìŠ¤íŠ¸ ì¡°í•©
    contexts = []
    for match in results.matches:
        metadata = match.metadata
        contexts.append(
            f"[{metadata['source']} - {metadata['page']}í˜ì´ì§€, {metadata['type']}]\n{metadata['text']}"
        )
    
    return "\n\n".join(contexts)

def get_ai_response(query, context):
    """OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±"""
    response = client.chat.completions.create(
        model="gpt-4-turbo-preview",
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "system", "content": f"ë‹¤ìŒì€ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œ ë‚´ìš©ì…ë‹ˆë‹¤:\n\n{context}"},
            {"role": "user", "content": query}
        ],
        temperature=0.7,
        max_tokens=2000
    )
    
    return response.choices[0].message.content

def initialize_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    if 'messages' not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”, K-Actuary AI Agentì…ë‹ˆë‹¤. ê¶ê¸ˆí•˜ì‹  ì ì´ ìˆìœ¼ì‹œë‹¤ë©´ ë§ì”€í•´ì£¼ì„¸ìš”."}
        ]

def main():
    st.set_page_config(
        page_title="K-Actuary AI Agent",
        page_icon="ğŸ¤–",
        layout="wide"
    )
    
    st.title("K-Actuary AI Agent")
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    initialize_session_state()
    
    # ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.write(message["content"])
    
    # ì‚¬ìš©ì ì…ë ¥
    if prompt := st.chat_input():
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.write(prompt)
        
        # AI ì‘ë‹µ ìƒì„±
        with st.chat_message("assistant"):
            with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
                context = query_pinecone(prompt)
                
                # AI ì‘ë‹µ ìƒì„±
                response = get_ai_response(prompt, context)
                
                st.write(response)
                st.session_state.messages.append({"role": "assistant", "content": response})

if __name__ == "__main__":
    main() 